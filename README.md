# Gemma_3n-offline-edtech

è®© **Gemma 3n** åœ¨ 0 ç½‘ç»œç°åœºå½“è€å¸ˆï¼ŒEdge TPU è¶…å……ï¼Œæ•™è‚²å¹³æƒã€‚

---

## âš¡ Quick Start (100 % ç¦»çº¿)

```bash
# 0. å…‹éš†ä»“åº“
git clone git@github.com:zhan2333/Gemma_3n-offline-edtech.git
cd Gemma_3n-offline-edtech          # â± <5 s

# 1. å®‰è£…/æ¿€æ´»ç¯å¢ƒï¼ˆâ‰ˆ2 minï¼‰
conda env create -f environment.yml
conda activate gemma3n

# 2. æ‹‰åŸºç¡€æ¨¡å‹ï¼ˆä¸€æ¬¡ 7 GBï¼Œå¯æ–­ç½‘å¤ç”¨ï¼‰
ollama pull gemma3n

# 3. è·å– LoRA æƒé‡ï¼ˆä»…å‡  MBï¼‰
mkdir -p models
wget -P models/ \
  https://huggingface.co/zhan23333/gemma3n-loras/resolve/main/skill_math_3n.lora

# 4. ç¦»çº¿æ¨ç†
python src/offline_infer.py --prompt "ä½ å¥½ï¼Œä¸–ç•Œï¼"
# â†’ ğŸ‘‹ Offline Gemma 3n å›åº”
```

---

## ğŸ“¦ ç›®å½•ç»“æ„

```
Gemma_3n-offline-edtech/
â”œâ”€ gemma3n-hf/              # â† export è„šæœ¬äº§ç‰©ï¼ˆ.gitignore å¿½ç•¥ï¼‰
â”œâ”€ models/                  # â† LoRA æƒé‡æ”¾è¿™é‡Œ
â”‚   â””â”€ skill_math_3n.lora
â”œâ”€ utils/
â”‚   â”œâ”€ export_gemma3n.py    # base â†’ HF ç›®å½•
â”‚   â””â”€ convert_tpu.py       # HF(+LoRA) â†’ ONNXâ†’TFLiteâ†’Edge-TPU
â”œâ”€ src/
â”‚   â””â”€ offline_infer.py     # ç¦»çº¿ CLI / API
â”œâ”€ data/                    # è®­ç»ƒæ ·ä¾‹ (jsonl)
â”œâ”€ environment.yml
â””â”€ README.md
```

---

## ğŸš€ åŠŸèƒ½æ¦‚è§ˆ

| æ¨¡å— | è¯´æ˜ |
| --- | --- |
| `utils/export_gemma3n.py` | è°ƒ `ollama.convert_to_hf()`ï¼ŒæŠŠ **gemma3n** æƒé‡å¯¼æˆæ ‡å‡† Hugging Face ç›®å½• |
| `train_lora.py` | ä½¿ç”¨ **UnsLoTH** æç®€å¾®è°ƒï¼Œäº§å‡º `.lora` æ–‡ä»¶ |
| `utils/convert_tpu.py` | ä¸€é”®ç®¡çº¿ï¼šmerge LoRA â†’ ONNX(fp16) â†’ TFLite(int8) â†’ *(å¯é€‰)* Edge-TPU `.tflite` |
| `src/offline_infer.py` | ç¦»çº¿ CLI/RESTï¼Œè‡ªåŠ¨æ£€æµ‹ `models/*.lora` |
| `kaggle_demo.ipynb` | Kaggle GPU ä¸Šå®Œæ•´å¤ç°ï¼šè§£å‹ HF â†’ å¾®è°ƒ â†’ é‡åŒ– |

---

## ğŸ é‡Œç¨‹ç¢‘

### 1ï¸âƒ£ LoRA å¿«é€Ÿå¾®è°ƒ

```bash
python train_lora.py \
  --base gemma3n-hf \
  --data data/math.jsonl \
  --epochs 1 --lr 1e-4
# è¾“å‡º models/skill_math_3n.lora
```

### 2ï¸âƒ£ Edge-TPU è½¬æ¢ & Benchmark

```bash
# å¯¼å‡º HF æƒé‡ï¼ˆä¸€æ¬¡æ€§ï¼‰
python utils/export_gemma3n.py --model gemma3n --out gemma3n-hf

# è½¬æ¢ / é‡åŒ–
python utils/convert_tpu.py \
  --base gemma3n-hf \
  --lora models/skill_math_3n.lora \
  --out  tpu_build
# æ—  edgetpu_compiler æ—¶è„šæœ¬ä¼šè‡ªåŠ¨è·³è¿‡ TPU ç¼–è¯‘å¹¶æç¤º
```
è‹¥æ—  edgetpu_compilerï¼Œè„šæœ¬è‡ªåŠ¨è·³è¿‡ TPU ç¼–è¯‘å¹¶æç¤ºã€‚

---

## ğŸ“Š ç»“æœç¤ºä¾‹

| Stage        | æ–‡ä»¶                             | Size |
|--------------|----------------------------------|------|
| merged fp32  | `merged/pytorch_model.bin`       | 6.9&nbsp;GB |
| ONNX fp16    | `onnx/model_fp16.onnx`           | 3.5&nbsp;GB |
| TFLite int8  | `gemma3n_int8.tflite`            | 1.6&nbsp;GB |
| Edge-TPU     | *(æ¨¡å‹è¶… 10 MBï¼Œä¸Š TPU å—é™)*    | â€” |

GPU T4 æ¨ç† â‰ˆ 71 tok/sï¼›Mac M3 CPU int8 â‰ˆ 7 tok/sã€‚

---

## ğŸ“ Tech Report & Demo

| é“¾æ¥          | å†…å®¹                                   |
|---------------|----------------------------------------|
| **Tech Report** | æ¶æ„ã€å¾®è°ƒæ—¥å¿—ã€Edge-TPU è½¬æ¢è®°å½•       |
| **3-min Video** | æ–­ç½‘æ¨ç†æ¼”ç¤º + Edge-TPU åŠ é€Ÿç”»é¢        |
| **HF Space**    | åœ¨çº¿ä½“éªŒï¼ˆçº¯ CPUï¼Œå¯é€‰ï¼‰               |

---

## ğŸ›  å¼€å‘è€…é€ŸæŸ¥

```bash
# å¯¼å‡º Hugging Face æƒé‡
python utils/export_gemma3n.py --model gemma3n --out gemma3n-hf

# ä¸€é”® ONNX / TFLite / Edge-TPU
python utils/convert_tpu.py --base gemma3n-hf --out tpu_build

# æœ¬åœ°æ¨ç†
python src/offline_infer.py --prompt "Explain Pythagorean theorem."
```

---

## ğŸ“œ License

ä»£ç  Apache-2.0ï¼›æ¨¡å‹æƒé‡éµå¾ª Google Gemma Licenseã€‚

*Made with â™¥ by zhan2333 â€” ä¸ºå¼±ç½‘æ•™è‚²åœºæ™¯æ‰“é€ çš„ç¦»çº¿ LLM å·¥å…·ç®±ã€‚*
